
> [!info] Info
> 
[Prompt-engineering Roadmap](https://roadmap.sh/prompt-engineering)

-  LLMs and How They Work
-  What is a Prompt?
-  What is Prompt Engineering?

## LLMs and How They Work

简单来说，**大语言模型（Large Language Model，简称 LLM）**就像是一个读过人类历史上几乎所有公开文字的“超级学生”。它不仅能背书，还能通过理解文字间的统计规律，学会逻辑推理、编写代码甚至进行文学创作。

要深入理解它，我们可以将其拆解为：**它是如何“阅读”的**、**它是如何“思考”的**，以及**它是如何“进化”的**。

---

## 1. 它是如何“阅读”的：Token 与向量化

机器并不认识汉字或单词，它们只认识数字。所以第一步是**数据转化**。

- **分词 (Tokenization)：** 模型会将一段话切分成一个个小块，称为 **Token**。一个 Token 可能是一个词、一个字，甚至是一个后缀。
- **嵌入 (Embedding)：** 每个 Token 会被转化成一串长长的数字列表（向量）。在这个“数学空间”里，意思相近的词（如“猫”和“狗”）距离会非常近，而无关的词（如“猫”和“手机”）距离较远。
- **位置编码（Positional Encoding）**：
- **数学转换 (pipeline)**：

![[introduction-tokenizer.png]]
### Tokenization

分词是模型读取提示词的第一步。它将文本拆分为“Token”（Token 可能是单词、子词或单个字符）。不同的模型使用的分词算法是不同的，目前大部分的模型大多都是基于 `BPE`和 `Unigram`两种算法。

>[!info] 
>常用的分词算法：  
>- BPE：从字节层级开始，迭代合并最频繁出现的字符对，形成词表。OpenAI基于BPE算法。
>- SentencePiece：将输入视为原始字节流，不依赖空格分词，包含 Unigram 和 BPE 两种子模式。Gemini基于此算法。

#### 第一步：分词 (Tokenization) —— 查字典

**动作：** 将连贯的句子“切”成一个个独立的语义单元，并换成数字。

想象一个巨大的**字典**（Vocabulary），里面有 50,000 个词，每个词都有一个唯一的身份证号（Index）。

- **输入：** `Data is oil`
- **动作：**
    1. 切割：`["Data", "is", "oil"]`
    2. 查表：
        - "Data" $\rightarrow$ ID: `452`
        - "is" $\rightarrow$ ID: `12`
        - "oil" $\rightarrow$ ID: `893`
- **数学形态：** 字符串 $\rightarrow$ 整数列表 `[452, 12, 893]`
- **核心逻辑：** 机器不认识字母，只认识数字 ID。

#### 第二步：嵌入 (Embedding) —— 赋予内涵

**动作：** 将平平无奇的整数 ID，变成蕴含丰富语义的**“一排数字”（向量）**。

想象一个巨大的**表格（矩阵 $W_E$）**。

- 表格的**行数**等于字典大小（如 50,000 行）。
- 表格的**列数**是模型的维度（$d_{model}$，比如 512 或 768）。

**动态演示：**

当 ID `452` ("Data") 进来时，就像一只机械手，去这个大表格里，把**第 452 行**的那一整排数字抓取出来。

- **输入：** `452`
- **数学转换：** 查表操作（Lookup）。
    $$E_{data} = \text{EmbeddingMatrix}[452]$$
    
- **输出：** 一个 512 维的向量，例如 `[0.12, -0.45, 0.99, ...]`。这个向量里的数字，就是机器对“Data”这个词特征的理解。

#### 第三步：位置编码 (Positional Encoding) —— 注入灵魂

**动作：** 告诉模型“谁在谁前面”。

这是一个关键点：Transformer 里的注意力机制（Self-Attention）是**并行**处理的。如果你不加位置信息，对模型来说 `Data is oil` 和 `oil is Data` 是一模一样的（就像一袋散乱的弹珠，没有顺序）。

我们需要给每个位置（第0个词、第1个词...）制造一个独特的“**位置指纹**”。

**数学转换（正弦/余弦波）：**

Transformer 使用不同频率的波形来生成这个指纹。对于位置 $pos$ 和维度 $i$：

$$\begin{aligned} PE_{(pos, 2i)} &= \sin(pos / 10000^{2i/d_{model}}) \\ PE_{(pos, 2i+1)} &= \cos(pos / 10000^{2i/d_{model}}) \end{aligned}$$

**动效想象：**

想象一个巨大的**波浪图**。

- **左边的维度**（$i$ 较小）：波浪频率很高，变化很快（像秒针）。
- **右边的维度**（$i$ 较大）：波浪频率很低，变化很慢（像时针）。

每个位置 $pos$ 都在这个波浪图上切一刀，切下来的那个截面，就是它的**位置向量**。

#### 第四步：加和 (Summation) —— 融合

**动作：** 将“语义”和“位置”合二为一。

这是最令人困惑的一步：为什么是**相加**（$+$）而不是拼接（Concatenation）？

- **拼接**会增加数据的维度（例如 512+512 = 1024），增加计算量。
- **相加**保留了维度（512），就像在原始的语义信号上，叠加了一个微弱的“位置底噪”。模型足够聪明，能学会把这两层信息剥离开来。

**数学转换：**

$$X_{final} = E_{word} + PE_{pos}$$

**最终状态：**

- 我们得到了一组矩阵，形状为 `[序列长度, 维度]` (例如 `[3, 512]`)。
- 这就是进入 Transformer 第一个 Attention 层的最终“燃料”。

---

## 2. 核心心脏：Transformer 与自注意力机制

这是 LLM 能够理解语境的“黑科技”。在 Transformer 架构出现之前，AI 很难处理长句子，因为它们容易忘掉前面的内容。

**自注意力机制 (Self-Attention)** 解决了这个问题。当模型处理一个词时，它会同时“观察”句子中所有的其他词，并计算它们之间的关联权重。

> **例子：** “他把**苹果**给了小明，因为它很**熟**。”
> 
> 当模型处理“熟”这个词时，注意力机制会给“苹果”分配极高的权重，而给“小明”分配极低的权重。这样模型就精确知道了谁才是“熟”的主语。

---

## 3. 它如何生成回答：概率接龙

LLM 的运作本质上是一个极其复杂的**概率预测游戏**。当你给它一个提示词（Prompt）时，它在做的事情是：**预测下一个最合理的 Token 是什么？**

1. **输入：** “人工智能的未来是...”
    
2. **计算：** 模型根据已有的百亿级参数，计算所有可能的下一个词的概率：
    
    - “光明” (45%)
        
    - “不确定” (20%)
        
    - “挑战” (10%)
        
3. **输出：** 它选择“光明”。
    
4. **循环：** 现在的输入变成了“人工智能的未来是光明”，它再继续预测下一个字，直到遇到停止符号。
    

---

## 4. 训练的三部曲：从“书呆子”到“全才”

要把一个模型训练得像 Gemini 或 GPT 这么智能，通常需要三个阶段：

|**阶段**|**过程**|**比喻**|
|---|---|---|
|**预训练 (Pre-training)**|在数万亿的网页、书籍数据上进行无监督学习。|**疯狂读书：** 读完图书馆所有书，但不知道对错。|
|**指令微调 (SFT)**|人类编写高质量的问答对，告诉模型什么样才是好的回答。|**导师指导：** 老师批改作业，教它如何回答问题。|
|**人类反馈强化学习 (RLHF)**|模型生成多个答案，由人类进行打分排序，优化它的偏好。|**性格塑造：** 纠正它的价值观，让它更有礼貌、更安全。|

---

## 5. 为什么它看起来有智能？（涌现能力）

当模型的参数规模（神经元连接数）达到一定临界点时（通常是百亿级），它会展现出一些原本没有被专门训练过的能力，比如：

- **零样本推理：** 没见过的题也能举一反三。
    
- **多步逻辑：** 能够处理复杂的数学证明。
    
- **上下文学习：** 你在对话里教它一个规则，它立刻就能学会。
    

这种现象在科学上被称为**“涌现” (Emergence)**。

---

### 技术背后的数学公式

如果你对底层的数学感兴趣，Transformer 中最核心的注意力机制可以用这个公式表示：

$$Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

其中 $Q, K, V$ 分别代表查询（Query）、键（Key）和值（Value），它们共同决定了模型在处理当前词时，应该分多少精力给上下文中的其他词。

**你想尝试用这些原理来优化你正在写的 Prompt（提示词），看看如何通过调整语境来改变模型的“概率预测”吗？**


## Sampling Parameters

## Output Control

## Repetition Penalties

## Structured Outputs

## Prompting Techniques

## Automatic Prompt Engineering
## AI Red Teaming